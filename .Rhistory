z=c(1:10)
z=z*2
summary(lm(y~x+offset(z)))
z=z*4
summary(lm(y~x+offset(z)))
z=z+5
summary(lm(y~x+offset(z)))
z=z+100
summary(lm(y~x+offset(z)))
z=z/10
summary(lm(y~x+offset(z)))
summary(lm(y~x+offset(z)))
z=z+log(10)
summary(lm(y~x+offset(z)))
z=z+log(10)
summary(lm(y~x+offset(z)))
999*.99
999-989.01
.99/1000
999*.99
999*.01
.99/1
.99/(.99+9.99)
install.packages("devtools")
library(devtools)
install_github('ramnathv/rCharts@dev')
install.packages("base64enc")
install_github('ramnathv/rCharts@dev')
x<-12:80
y<-(x/2)+1
plot(x,y)
48/2
25/2
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("caret")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names(concrete)
plot(1:nrow(concrete),concrete$CompressiveStrength)
plot(1:nrow(concrete),concrete$CompressiveStrength,col=concrete$FlyAsh)
str(concrete)
plot(1:nrow(concrete),concrete$CompressiveStrength)
plot(1:nrow(concrete),concrete$CompressiveStrength,col=concrete$FlyAsh)
plot(1:nrow(concrete),concrete$CompressiveStrength,col=concrete$Age)
plot(1:nrow(concrete),training$CompressiveStrength)
plot(1:nrow(training),training$CompressiveStrength)
plot(1:nrow(training),training$CompressiveStrength,col=training$Age)
plot(1:nrow(training),training$CompressiveStrength,col=training$FlyAsh)
summary(training$FlyAsh)
plot(1:nrow(training),training$CompressiveStrength,col=training$Cement)
plot(1:nrow(training),training$CompressiveStrength,col=training$BlastFurnaceSlag)
plot(1:nrow(training),training$CompressiveStrength,col=training$Water)
plot(1:nrow(training),training$CompressiveStrength,col=training$Superplasticizer)
plot(1:nrow(training),training$CompressiveStrength,col=CoarseAggregate)
plot(1:nrow(training),training$CompressiveStrength,col=training$CoarseAggregate)
plot(1:nrow(training),training$CompressiveStrength,col=training$FineAggregate)
plot(1:nrow(training),training$CompressiveStrength,col=training$Age)
library(ggplot2)
library(ggplot2)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$index<-1:nrow(training)
names(concrete)
ggplot(training, aes(x=index, y=CompressiveStrength, color=FlyAsh)) +
geom_point(shape=1)
ggplot(training, aes(x=index, y=CompressiveStrength, color=Age)) +
geom_point(shape=1)
ggplot(training, aes(x=index, y=CompressiveStrength, color=log(Age))) +
geom_point(shape=1)
ggplot(training, aes(x=Superplasticizer, y=CompressiveStrength, color=log(Age))) +
geom_jitter(shape=1)
summary(concrete$Superplasticizer)
log(0)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
which(grepl("IL_",names(training)))
names(training)[il]
il<-which(grepl("IL_",names(training)))
names(training)[il]
il<-which(grepl("^IL_",names(training)))
names(training)[il]
preProc<-preProcess(trining[,il],method="pca",pcaComp=5)
preProc<-preProcess(training[,il],method="pca",pcaComp=5)
preProc
trainPC<-predict(preProc,training[,il])
trainPC
M<-abs(cor(trainPC))
M
il
length(il)
preProc<-preProcess(training[,il],method="pca",pcaComp=12)
trainPC<-predict(preProc,training[,il])
M<-abs(cor(trainPC))
M
trainPC
M<-abs(cor(trainPC))
M
com<-prcomp(training[,il])
com
names(com)
summary(a)
summary(com)
0.3523+ 0.2397 +0.2190 +0.06157+ 0.04663+ 0.02529+ 0.02071 +0.0144 +0.01018
0.3523+ 0.2397 +0.2190 +0.06157+ 0.04663+ 0.02529+ 0.02071 +0.0144
0.3523+ 0.2397 +0.2190 +0.06157+ 0.04663+ 0.02529+ 0.02071
0.3523+ 0.2397 +0.2190 +0.06157+ 0.04663+ 0.02529
0.3523+ 0.2397 +0.2190 +0.06157+ 0.04663
0.3523+ 0.2397 +0.2190 +0.06157
0.3523+ 0.2397 +0.2190
0.3523+ 0.2397
names(training)
trainings<-training[,c(1,il)]
modelFit1<-train(trainings$diagnosis~.,method="glm",preProcess="pca",data=trainings)
modelFit2<-train(trainings$diagnosis~.,method="glm",data=trainings)
install.packages("e1071")
trainings<-training[,c(1,il)]
testings<-testing[,c(1,il)]
modelFit1<-train(trainings$diagnosis~.,method="glm",preProcess="pca",data=trainings)
modelFit2<-train(trainings$diagnosis~.,method="glm",data=trainings)
confusionMatrix(testings$diagnosis,predict(modelFit1,testings))
confusionMatrix(testings$diagnosis,predict(modelFit2,testings))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
il<-which(grepl("^IL_",names(training)))
training[il]
names(training)[il]
com<-prcomp(training[,il])
summary(com)
com<-prcomp(AlzheimerDisease[,il])
com<-prcomp(adData[,il])
com
summary(com)
preProc<-preProcess(training[,il],method="pca",thresh=0.9)
preproc$pcaComp
preProc$pcaComp
preProc$numComp
preProc<-preProcess(training[,il],method="pca",thresh=0.8)
preProc$numComp
library(devtools)
instal_github("slidify","ramnathv")
install_github("slidify","ramnathv")
install_github("slidifyLibraries","ramnathv")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
AppliedPredictiveModeling
segmentationOriginal
names(segmentationOriginal)
segmentationOriginal$case
segmentationOriginal$Case
test <- segmentationOriginal[segmentationOriginal$Case == "Test"]
train <- segmentationOriginal[segmentationOriginal$Case == "Train"]
names(segmentationOriginal)
test <- segmentationOriginal[segmentationOriginal$Case == "Test", ]
train <- segmentationOriginal[segmentationOriginal$Case == "Train", ]
set.seed(145)
set.seed(125)
test$class
test$Class
set.seed(125)
modFit <- train(Class ~.,method='rpart',data=train)
print(modFit$finalModel)
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
fancyRpartPlot(modFit$finalModel)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
olive
modFit <- train(Area ~.,method='rpart',data=olive)
library(pgmm)
data(olive)
olive = olive[,-1]
modFit <- train(Area ~.,method='rpart',data=olive)
print(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata = as.data.frame(t(colMeans(olive))))
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages(‘ElemStatLearn’)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
names(trainSA)
set.seed(13234)
modFit <- train(cdh ~age+alcohol+obesity+tobacco+typea+ldl,
method='glm',family="binomial",data=trainSA)
print(modFit$finalModel)
set.seed(13234)
modFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,
method='glm',family="binomial",data=trainSA)
print(modFit$finalModel)
predict(modFit,newdata = trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
pre<- predict(modFit,newdata = trainSA)
val<- trainSA$chd
missClass(val,pre)
pre<- predict(modFit,newdata = testSA)
val<- testSA$chd
missClass(val,pre)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
set.seed(33833)
modFit <- train(y ~., method='rf',data=vowel.train)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modFit <- train(y ~., method='rf',data=vowel.train)
print(modFit$finalModel)
modFit
names(modFit)
names(modFit$modelInfo)
names(modFit$method)
names(modFit$modelType)
names(modFit$pred)
names(modFit$metric)
modFit$metric
set.seed(33833)
modFit <- train(y ~., method='rf',data=vowel.train,proximity=T)
print(modFit$finalModel)
varImp(modFit)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
set.seed(33833)
modFit <- train(y ~., method='rf',data=vowel.train, importance = TRUE)
varImp(modFit)
varImp(modFit$finalModel)
dd<-randomForest(y ~., method='rf',data=vowel.train, importance = TRUE)
varImp(dd)
set.seed(33833)
dd<-randomForest(y ~., method='rf',data=vowel.train, importance = TRUE)
varImp(dd)
dd<-randomForest(y ~., method='rf',data=vowel.train)
varImp(dd)
set.seed(33833)
set.seed(33833)
modFit <- train(y ~., method='rf',data=vowel.train)
varImp(modFit$finalModel)
order(varImp(modFit$finalModel))
vI <- varImp(modFit$finalModel)
vI$sample <- row.names(vI); vI <- vI[order(vI$Overall, decreasing = T),]
vI
set.seed(33833)
modFit <- train(y ~., method='rf',data=vowel.train,prox=TRUE)
varImp(modFit$finalModel)
vI <- varImp(modFit$finalModel)
vI$sample <- row.names(vI); vI <- vI[order(vI$Overall, decreasing = T),]
vI
setInternet2(TRUE)
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile = "train.csv")
library(data.table)
data <- fread("train.csv")
names(data)
summary(data)
data[,kurtosis_picth_forearm]
as.numeric(data[,kurtosis_picth_forearm])
str(data)
data[, names(data)[1:7] := NULL, with = F]
str data
str (data)
data[,min_yaw_dumbbell]
str(data)
data[, lapply(.SD, as.character)]
str(data)
We transform columns to numeric (except outcome), since some columns have been read as character
data <- data[, lapply(.SD, as.numeric), .SDcols = -160]
dim(data)
names(data)
data <- fread("train.csv")
data[, names(data)[1:7] := NULL, with = F]
data <- data[, lapply(.SD, as.numeric), .SDcols = -153]
str(data)
to.drop <- data[,lapply(.SD, is.na)]
to.drop
to.drop <- data[,lapply(.SD, f(x) sum(is.na(x)))]
to.drop <- data[,lapply(.SD, function(x) sum(is.na(x)))]
to.drop
to.drop <- data[, sum(is.na(.SD)))]
to.drop <- data[, sum(is.na(.SD))]
to.drop
to.drop <- data[,apply(.SD, function(x) sum(is.na(x)))]
to.drop <- data[,sapply(.SD, function(x) sum(is.na(x)))]
to.drop
to.drop <- to.drop < 0.70*(nrow(data))
to.drop
to.drop <- data[,sapply(.SD, function(x) sum(is.na(x)))]
to.drop <- to.drop > 0.70*(nrow(data))
to.drop
data[, names(data)[to.drop] := NULL, with = F]
str(data)
dim(data)
names(data)
data <- fread("train.csv")
data[, names(data)[1:7] := NULL, with = F]
names(data)
data <- data[, lapply(.SD, as.numeric), .SDcols = -153]
names(data)
data <- fread("train.csv")
data[, names(data)[1:7] := NULL, with = F]
names(data)
outcome <- data[, classe]
data <- data[, lapply(.SD, as.numeric), .SDcols = -153]
data[, classe := outcome]
nameS(data)
names(data)
to.drop <- data[,sapply(.SD, function(x) sum(is.na(x)))]
to.drop <- to.drop > 0.70*(nrow(data))
data[, names(data)[to.drop] := NULL, with = F]
dim(data)
data <- fread("train.csv")
data[, names(data)[1:7] := NULL, with = F]
outcome <- as.factor(data[, classe])
data <- data[, lapply(.SD, as.numeric), .SDcols = -153]
data[, classe := outcome]
to.drop <- data[,sapply(.SD, function(x) sum(is.na(x)))]
to.drop <- to.drop > 0.70*(nrow(data))
data[, names(data)[to.drop] := NULL, with = F]
summary(data[, classe])
library(caret)
set.seed(1234)
library(caret)
set.seed(1234)
train.index <- createDataPartition(y = data[, classe], p = 0.6, list = F)
training <- data[train.index, ]
testing <- data[-train.index, ]
train.index <- createDataPartition(y = data[, classe], p = 0.6, list = F)
train.index
train.index <- createDataPartition(y = data[, classe], p = 0.6, list = T)
train.index
str(train.index)
train.index[[1]]
class(train.index[[1]])
set.seed(1234)
train.index <- createDataPartition(y = data[, classe], p = 0.6, list = T)
training <- data[train.index[[1]], ]
testing <- data[-train.index[[1]], ]
M <- abs(cor(training[, -53]))
M <- abs(cor(training[, .SD, .SDcols = -53]))
diag(M) <- 0
which(M > 0.8, arr.ind = T)
which(M > 0.9, arr.ind = T)
library(corrplot)
install.packages("corrplot")
library(corrplot)
corrplot.mixed(M)
M <- (cor(training[, .SD, .SDcols = -53]))
corrplot.mixed(M)
library(ggplot2)
library(reshape)
z.m <- melt(training[, .SD, .SDcols = -53])
ggplot(z.m, aes(X1, X2, fill = value)) + geom_tile() +
scale_fill_gradient2(low = "blue",  high = "yellow")
install.packages("reshape")
library(ggplot2)
library(reshape)
z.m <- melt(training[, .SD, .SDcols = -53])
ggplot(z.m, aes(X1, X2, fill = value)) + geom_tile() +
scale_fill_gradient2(low = "blue",  high = "yellow")
library(ggplot2)
library(reshape)
M.m <- melt(M)
ggplot(M.m, aes(X1, X2, fill = value)) + geom_tile() +
scale_fill_gradient2(low = "blue",  high = "yellow")
ggplot(M.m, aes(X1, X2, fill = value)) + geom_tile("LL") +
scale_fill_gradient2(low = "blue",  high = "yellow")
library(ggplot2)
library(reshape)
M.m <- melt(M)
ggplot(M.m, aes(X1, X2, fill = value)) + geom_tile() +
scale_fill_gradient2(low = "blue",  high = "yellow") + ggtitle("Correlation matrix of predictors")
ggplot(M.m, aes(X1, X2, fill = value)) + geom_tile() +
scale_fill_gradient2(low = "blue",  high = "yellow") + ggtitle("Correlation matrix of predictors") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
preProc <- preProcess(training[, .SD, .SDcols = -53], method = 'pca', thresh = 0.95)
trainPC <- predict(preProc, training[, .SD, .SDcols = -53])
preProc
names(preProc)
pcaComp
preProc$pcaComp
preProc$pcaComp
preProc
trainPC
preProc <- preProcess(training[, .SD, .SDcols = -53], method = 'pca', thresh = 0.95)
preProc
names(training)
trainPC <- predict(preProc, training[, .SD, .SDcols = -53])
set.seed(999)
modelFit <- train(training[, classe] ~., method = "rf", data = trainPC)
trainPC <- predict(preProc, training[, .SD, .SDcols = -53])
set.seed(999)
modelFit <- train(training[, classe] ~., method = "rf", data = trainPC, proximity = F)
set.seed(999)
trainPC <- predict(preProc, training[, .SD, .SDcols = -53])
fitControl <- trainControl(method = "cv",
number = 10)
modelFit <- train(training[, classe] ~., method = "rf", data = trainPC,
proximity = F, trControl = fitControl)
modelFit
names(modelFit)
modelFit$pred
confusionMatrix(testing[, classe], predict(modelFit, trainPC))
confusionMatrix(training[, classe], predict(modelFit, trainPC))
m <- confusionMatrix(training[, classe], predict(modelFit, trainPC))
names(m)
m$positive
m$table
m$overall
testPC <- predict(preProc, testing[, .SD, .SDcols = -53])
confusionMatrix(testing[, classe], predict(modelFit, testPC))
getwd()
setwd("C:\\Users\\AndresAlejandro\\Desktop\\specialization\\8- Practical machine learning\\project")
rr <- fread("test.csv")
names(rr)
rr[, names(rr)[1:7] := NULL, with = F]
rr
dim(rr)
rr <- rr[, lapply(.SD, as.numeric), .SDcols = -153]
rr
to.drop
rr[, names(data)[to.drop] := NULL, with = F]
rr
dim(dd)
dim(rr)
rr[, names(rr)[to.drop] := NULL, with = F]
answers = rep("A", 20)
dim(rr)
names(rr)==names(data[,.SD,.SDcols=-53])
rrPC <- predict(preProc, rr)
predict(modelFit, rrPC)
answers<-predict(modelFit, rrPC)
answers
class(answers)
answers<-as.character(answers)
answers
answers = rep("A", 20)
answers<-as.character(answers)
rrPC <- predict(preProc, rr)
answers<-predict(modelFit, rrPC)
answers<-as.character(answers)
answers
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(answers)
testPC <- predict(preProc, testing[, .SD, .SDcols = -53])
confusionMatrix(testing[, classe], predict(modelFit, testPC))
